ğŸ”¹ ğŸ§© Module 1: Browser Automation + Raw Scraper
Status: âœ… Done

âœ”ï¸ Built using Playwright (Python) for headless browser control.

âœ”ï¸ Implemented launch_browser_and_scrape_opportunities() in browser_driver.py.

âœ”ï¸ Script navigates to a specified Gowin URL and extracts:

title

description

link

âœ”ï¸ Selector-based scraping with error handling, screenshots on failure.

âœ”ï¸ Successfully tested end-to-end from VS Code in virtual environment.

âœ… Output returned: list of dicts (opportunities).

ğŸ”¹ ğŸ§© Module 2: Conditional Filtering Engine
Status: âœ… Logic shared & to be implemented

ğŸ“¦ Built as opportunity_parser.py

âœ”ï¸ Accepts raw data (from the scraper)

âœ”ï¸ Loads external filters.json for logic (keyword-based logic, tag fit levels)

âœ”ï¸ Applies logic:

Best Fit â†’ strict match with core keywords

Moderate â†’ partial match with related tech

Ignore â†’ eliminate noisy domains like fashion/retail

âœ”ï¸ Returns only relevant structured opportunities with a new "fit" tag. 

Module 3: Data Structurer & Exporter
Structure final results to structured_output.csv or .json

Add timestamping, source tagging, etc.

ğŸ”¸ Module 4: Email Dispatch
Use SMTP with .env credentials

Send daily filtered reports to respective POCs

Attach opportunity links and metadata

ğŸ”¸ Module 5: Trigger Engine / Scheduler
Auto-trigger the scraper every day (9-hour window)

Use schedule, cron, or Task Scheduler (on Windows)

ğŸ”¸ Module 6: Logging + Error Tracking
Create logs/run_log_<date>.log

Track start time, scrape count, success/failure, exceptions